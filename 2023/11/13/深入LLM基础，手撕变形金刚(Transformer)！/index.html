<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>深入LLM基础，手撕变形金刚(Transformer)！ - lcz的技术小窝</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="深入LLM基础，手撕变形金刚(Transformer)！在过去的两年里，我们见证了以ChatGPT为代表的大语言模型，也就是所谓的LLM给整个AI领域带来的巨大变革，可以说LLM杀死了NLP比赛，却又让NLP从此重获新生。那么LLM的核心原理是什么？要实现一个基础的LLM需要什么技术储备？” 本文将带领大家深入探讨LLM的技术本质，并通过手写实现来加深理解。需要说明的是，虽然现代LLM都是数千亿参">
<meta property="og:type" content="article">
<meta property="og:title" content="lcz的技术小窝">
<meta property="og:url" content="http://example.com/2023/11/13/%E6%B7%B1%E5%85%A5LLM%E5%9F%BA%E7%A1%80%EF%BC%8C%E6%89%8B%E6%92%95%E5%8F%98%E5%BD%A2%E9%87%91%E5%88%9A(Transformer)%EF%BC%81/index.html">
<meta property="og:site_name" content="lcz的技术小窝">
<meta property="og:description" content="深入LLM基础，手撕变形金刚(Transformer)！在过去的两年里，我们见证了以ChatGPT为代表的大语言模型，也就是所谓的LLM给整个AI领域带来的巨大变革，可以说LLM杀死了NLP比赛，却又让NLP从此重获新生。那么LLM的核心原理是什么？要实现一个基础的LLM需要什么技术储备？” 本文将带领大家深入探讨LLM的技术本质，并通过手写实现来加深理解。需要说明的是，虽然现代LLM都是数千亿参">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-13T10:54:12.678Z">
<meta property="article:modified_time" content="2023-11-13T10:54:12.678Z">
<meta property="article:author" content="lcz的技术小窝">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">lcz的技术小窝</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">主页</a>
        
          <a class="main-nav-link" href="/archives">文章列表</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-深入LLM基础，手撕变形金刚(Transformer)！" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/11/13/%E6%B7%B1%E5%85%A5LLM%E5%9F%BA%E7%A1%80%EF%BC%8C%E6%89%8B%E6%92%95%E5%8F%98%E5%BD%A2%E9%87%91%E5%88%9A(Transformer)%EF%BC%81/" class="article-date">
  <time class="dt-published" datetime="2023-11-13T10:54:12.678Z" itemprop="datePublished">2023-11-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="深入LLM基础，手撕变形金刚-Transformer-！"><a href="#深入LLM基础，手撕变形金刚-Transformer-！" class="headerlink" title="深入LLM基础，手撕变形金刚(Transformer)！"></a>深入LLM基础，手撕变形金刚(Transformer)！</h1><p>在过去的两年里，我们见证了以ChatGPT为代表的大语言模型，也就是所谓的LLM给整个AI领域带来的巨大变革，可以说LLM杀死了NLP比赛，却又让NLP从此重获新生。那么LLM的核心原理是什么？要实现一个基础的LLM需要什么技术储备？”</p>
<p>本文将带领大家深入探讨LLM的技术本质，并通过手写实现来加深理解。需要说明的是，虽然现代LLM都是数千亿参数的庞然大物，但它们的基石仍然是Transformer架构。理解并能够实现Transformer，是深入LLM领域的必经之路。</p>
<h2 id="1-从序列建模说起"><a href="#1-从序列建模说起" class="headerlink" title="1. 从序列建模说起"></a>1. 从序列建模说起</h2><p>在深入技术细节之前，我们需要理解一个基本问题：为什么需要Transformer？</p>
<p>在Transformer出现之前，处理序列数据主要依赖RNN(循环神经网络)及其变体LSTM、GRU等。这类模型通过循环结构来处理序列信息，但存在三个主要问题：</p>
<ol>
<li>无法并行计算，处理长序列时计算效率低下</li>
<li>长距离依赖问题，即难以捕捉序列中相距较远的关联</li>
<li>梯度消失&#x2F;爆炸问题，影响模型训练稳定性</li>
</ol>
<p>2017年，Google提出的Transformer架构巧妙地解决了这些问题。它抛弃了循环结构，完全基于注意力机制构建，实现了：</p>
<ul>
<li>充分的并行计算</li>
<li>直接的全局信息交互  </li>
<li>稳定的梯度传播</li>
</ul>
<p>这些优势使得Transformer迅速成为处理序列数据的首选架构。</p>
<h2 id="2-Transformer架构"><a href="#2-Transformer架构" class="headerlink" title="2. Transformer架构"></a>2. Transformer架构</h2><p>Transformer采用了经典的编码器-解码器(Encoder-Decoder)架构，但内部实现非常独特：</p>
<ul>
<li><p><strong>编码器</strong>:将输入序列编码为隐藏表示</p>
<ul>
<li>包含6层相同的结构</li>
<li>每层包含自注意力机制和前馈网络</li>
<li>采用残差连接和层归一化</li>
</ul>
</li>
<li><p><strong>解码器</strong>:基于编码器的输出生成目标序列</p>
<ul>
<li>同样包含6层相同的结构</li>
<li>每层包含自注意力、交叉注意力和前馈网络</li>
<li>使用mask机制确保自回归生成</li>
</ul>
</li>
</ul>
<p>核心创新点在于抛弃了循环结构，转而使用自注意力机制实现序列建模。这一设计使得模型可以：</p>
<ol>
<li>并行处理整个序列</li>
<li>建立任意位置间的直接联系</li>
<li>控制信息流动的粒度</li>
</ol>
<p>接下来，让我们深入了解注意力机制的工作原理。</p>
<h2 id="3-注意力机制-核心中的核心"><a href="#3-注意力机制-核心中的核心" class="headerlink" title="3. 注意力机制:核心中的核心"></a>3. 注意力机制:核心中的核心</h2><h3 id="3-1-注意力机制的直观理解"><a href="#3-1-注意力机制的直观理解" class="headerlink" title="3.1 注意力机制的直观理解"></a>3.1 注意力机制的直观理解</h3><p>在解释数学原理之前，先用一个直观的类比来理解注意力机制：</p>
<p>想象你正在阅读一篇长文章，要给出文章大意。你会怎么做？</p>
<ol>
<li>先快速浏览全文，找到关键词和重要句子 (Query过程)</li>
<li>将当前关注点与这些关键信息进行对比 (Key匹配过程)  </li>
<li>重点关注相关度高的内容，忽略无关内容 (Value加权过程)</li>
</ol>
<p>注意力机制本质上就是在模拟这个过程。</p>
<h3 id="3-2-数学原理"><a href="#3-2-数学原理" class="headerlink" title="3.2 数学原理"></a>3.2 数学原理</h3><p>形式化地说，注意力机制包含三个核心步骤：</p>
<ol>
<li><strong>线性映射</strong>:将输入向量X转换为查询(Q)、键(K)、值(V)三组向量：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Q = XW_q</span><br><span class="line">K = XW_k  </span><br><span class="line">V = XW_v</span><br></pre></td></tr></table></figure>

<ol start="2">
<li><strong>相似度计算</strong>: 使用点积计算Q和K的相似度：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">S = QK^T / sqrt(d_k)</span><br></pre></td></tr></table></figure>

<ol start="3">
<li><strong>加权聚合</strong>: 使用softmax归一化后的分数加权V：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Attention(Q,K,V) = softmax(QK^T/sqrt(d_k))V</span><br></pre></td></tr></table></figure>

<p>这里的<code>sqrt(d_k)</code>是一个缩放因子，用于调节梯度，防止内积结果过大导致softmax梯度消失。</p>
<h3 id="3-3-多头注意力机制"><a href="#3-3-多头注意力机制" class="headerlink" title="3.3 多头注意力机制"></a>3.3 多头注意力机制</h3><p>为了增强模型的表达能力，Transformer使用了多头注意力机制，就像是三头六臂的哪吒，头变多了，模型就变强了。简单说就是：</p>
<ol>
<li>将Q、K、V分别投影到h个子空间</li>
<li>在每个子空间独立计算注意力</li>
<li>合并h个头的输出结果</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, num_heads</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % num_heads == <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.d_k = d_model // num_heads</span><br><span class="line">        <span class="variable language_">self</span>.num_heads = num_heads</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 为每个头创建独立的投影矩阵</span></span><br><span class="line">        <span class="variable language_">self</span>.w_q = nn.Linear(d_model, d_model)</span><br><span class="line">        <span class="variable language_">self</span>.w_k = nn.Linear(d_model, d_model)</span><br><span class="line">        <span class="variable language_">self</span>.w_v = nn.Linear(d_model, d_model)</span><br><span class="line">        <span class="variable language_">self</span>.w_o = nn.Linear(d_model, d_model)</span><br></pre></td></tr></table></figure>

<p>多头机制让模型可以:</p>
<ul>
<li>关注不同的特征模式</li>
<li>在不同的表示子空间建模</li>
<li>集成多个角度的信息</li>
</ul>
<h2 id="4-位置编码-序列顺序的守护者"><a href="#4-位置编码-序列顺序的守护者" class="headerlink" title="4. 位置编码:序列顺序的守护者"></a>4. 位置编码:序列顺序的守护者</h2><p>Transformer抛弃循环结构带来了并行计算的优势，但也失去了序列的位置信息。为了补充这一信息，Transformer引入了位置编码(Positional Encoding)。</p>
<h3 id="4-1-正弦位置编码"><a href="#4-1-正弦位置编码" class="headerlink" title="4.1 正弦位置编码"></a>4.1 正弦位置编码</h3><p>原始论文采用正弦和余弦函数的组合:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_positional_encoding</span>(<span class="params">seq_len, d_model</span>):</span><br><span class="line">    pe = torch.zeros(seq_len, d_model)</span><br><span class="line">    pos = torch.arange(<span class="number">0</span>, seq_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line">    div = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) * </span><br><span class="line">                   -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">    </span><br><span class="line">    pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(pos * div)</span><br><span class="line">    pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(pos * div)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> pe</span><br></pre></td></tr></table></figure>

<p>这种设计有几个优点：</p>
<ol>
<li>可以处理任意长度的序列</li>
<li>位置之间的相对关系容易计算</li>
<li>可以平滑地扩展到未见过的位置</li>
</ol>
<h3 id="4-2-改进"><a href="#4-2-改进" class="headerlink" title="4.2 改进"></a>4.2 改进</h3><p>不过在实践中，研究者们提出了多种改进方案：</p>
<ol>
<li><strong>RoPE(Rotary Position Embedding)</strong></li>
</ol>
<ul>
<li>通过复数旋转引入位置信息</li>
<li>具有更好的相对位置建模能力</li>
</ul>
<ol start="2">
<li><strong>ALiBi(Attention with Linear Biases)</strong></li>
</ol>
<ul>
<li>为注意力分数添加线性偏置</li>
<li>外推能力更强</li>
</ul>
<ol start="3">
<li><strong>绝对位置编码</strong></li>
</ol>
<ul>
<li>直接学习每个位置的嵌入</li>
<li>实现简单，在某些任务上表现不错</li>
</ul>
<h2 id="5-规范化与残差-训练的稳定器"><a href="#5-规范化与残差-训练的稳定器" class="headerlink" title="5. 规范化与残差:训练的稳定器"></a>5. 规范化与残差:训练的稳定器</h2><h3 id="5-1-Layer-Normalization"><a href="#5-1-Layer-Normalization" class="headerlink" title="5.1 Layer Normalization"></a>5.1 Layer Normalization</h3><p>Layer Normalization在Transformer中起着至关重要的作用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LayerNorm</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, eps=<span class="number">1e-6</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.gamma = nn.Parameter(torch.ones(features))</span><br><span class="line">        <span class="variable language_">self</span>.beta = nn.Parameter(torch.zeros(features))</span><br><span class="line">        <span class="variable language_">self</span>.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        mean = x.mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.gamma * (x - mean) / (std + <span class="variable language_">self</span>.eps) + <span class="variable language_">self</span>.beta</span><br></pre></td></tr></table></figure>

<p>它的作用是：</p>
<ol>
<li>标准化每一层的输出</li>
<li>稳定梯度传播</li>
<li>加速训练收敛</li>
</ol>
<h3 id="5-2-残差连接"><a href="#5-2-残差连接" class="headerlink" title="5.2 残差连接"></a>5.2 残差连接</h3><p>残差连接的实现非常简单：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualConnection</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.norm = LayerNorm(size)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, sublayer</span>):</span><br><span class="line">        <span class="keyword">return</span> x + <span class="variable language_">self</span>.dropout(sublayer(<span class="variable language_">self</span>.norm(x)))</span><br></pre></td></tr></table></figure>

<p>但它带来的好处是巨大的：</p>
<ol>
<li>缓解梯度消失问题</li>
<li>允许信息直接传递</li>
<li>使得网络可以更深</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/11/13/%E6%B7%B1%E5%85%A5LLM%E5%9F%BA%E7%A1%80%EF%BC%8C%E6%89%8B%E6%92%95%E5%8F%98%E5%BD%A2%E9%87%91%E5%88%9A(Transformer)%EF%BC%81/" data-id="cmhesos9e000544ur7vzx72x6" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/11/13/Go%20vs%20Java%E6%B7%B1%E5%85%A5%E5%AF%B9%E6%AF%94%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          (no title)
        
      </div>
    </a>
  
  
    <a href="/2023/11/13/%E5%BA%93%E5%AD%98%E7%83%AD%E7%82%B9%E9%97%AE%E9%A2%98%EF%BC%8C%E5%A6%82%E4%BD%95%E7%94%A8Redis%E6%89%9B%E4%BD%8F%E7%99%BE%E4%B8%87QPS%EF%BC%9F/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title"></div>
    </a>
  
</nav>

  
</article>


</section>
        
<aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/shiro-mockmvc/" rel="tag">shiro mockmvc</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/shiro-mockmvc/" style="font-size: 10px;">shiro mockmvc</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list">
        <li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/06/">June 2024</a></li>
        <li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/05/">May 2024</a></li>
        <li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a></li>
        <li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/03/">March 2023</a></li>
        <li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a></li>
      </ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/06/21/Go%20vs%20Java%E6%B7%B1%E5%85%A5%E5%AF%B9%E6%AF%94%E7%B3%BB%E5%88%97%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9A%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/">Go vs Java深入对比系列（四）：并发编程</a>
          </li>
          <li>
            <a href="/2024/06/19/Go%20vs%20Java%E6%B7%B1%E5%85%A5%E5%AF%B9%E6%AF%94%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86/">Go vs Java深入对比系列（三）：错误处理</a>
          </li>
          <li>
            <a href="/2024/06/10/Go%20vs%20Java%E6%B7%B1%E5%85%A5%E5%AF%B9%E6%AF%94%E7%B3%BB%E5%88%97%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E7%B1%BB%E5%9E%8B%E7%B3%BB%E7%BB%9F/">Go vs Java深入对比系列（二）：类型系统</a>
          </li>
          <li>
            <a href="/2024/05/30/Go%20vs%20Java%E6%B7%B1%E5%85%A5%E5%AF%B9%E6%AF%94%E7%B3%BB%E5%88%97%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9A%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/">Go vs Java深入对比系列（一）：语言设计哲学</a>
          </li>
          <li>
            <a href="/2023/11/13/%E6%B7%B1%E5%85%A5LLM%E5%9F%BA%E7%A1%80%EF%BC%8C%E6%89%8B%E6%92%95%E5%8F%98%E5%BD%A2%E9%87%91%E5%88%9A(Transformer)%EF%BC%81/">深入LLM基础，手撕变形金刚(Transformer)！</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 lcz的技术小窝<br>
      Powered by <a href="https://hexo.io/" target="_blank">lcz的技术小窝</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">主页</a>
  
    <a href="/archives" class="mobile-nav-link">文章列表</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>